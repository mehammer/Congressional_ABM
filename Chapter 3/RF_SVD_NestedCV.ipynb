{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMYIhQqfQ5lOWr2IAaVkp07"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"VfZNy7ZeGGAl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764098022899,"user_tz":300,"elapsed":4733,"user":{"displayName":"Michael Hammer","userId":"02778246655929855865"}},"outputId":"35fcd453-a1d0-462f-891a-f8ec97c710de"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}],"source":["##import and format data\n","import pandas as pd\n","import numpy as np\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.decomposition import PCA\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","nltk.download('wordnet')\n","nltk.download('punkt')\n","\n","#Upload complete training dataset\n","all114 = pd.read_csv('training_data_114_final.csv', dtype=\"string\")\n","all2 = all114.astype({'nominate_dim1':'float', 'nominate_dim2': 'float'})\n","next114 = all2.dropna()\n","\n","###variable of NOMINATE values\n","y = next114.nominate_dim1\n","y1 = next114.nominate_dim2\n","\n","# OR Upload LEMMATIZED dataset\n","all114 = pd.read_csv('lemmatized_output.csv', dtype=\"string\")\n","docs = all114['speech']\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iozZpiKEG6DR"},"outputs":[],"source":["#upload and run the custom stopword list\n","from congress_stopwords import congress\n","\n"]},{"cell_type":"code","source":["# Step 4: Convert text to TF-IDF matrix and make it dense\n","\n","\n","vectorizer = TfidfVectorizer(stop_words=congress, min_df=5, max_df=0.5)\n","X_sparse = vectorizer.fit_transform(docs)\n","\n","\n","print(f\"TF-IDF shape before PCA: {X_sparse.shape}\")"],"metadata":{"id":"wAbN7VtLOkS7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["DIMENSION 1"],"metadata":{"id":"uwTUnr-TMNU5"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import KFold, RandomizedSearchCV\n","from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, max_error, make_scorer\n","\n","# --- Your data ---\n","# Replace these with your actual data\n","X = X_sparse\n","y = y\n","\n","# --- Define pipeline: SVD + Random Forest ---\n","pipeline = Pipeline([\n","    ('svd', TruncatedSVD(random_state=42)),\n","    ('rf', RandomForestRegressor(random_state=42))\n","])\n","\n","# --- Define hyperparameter space ---\n","param_grid = {\n","    'svd__n_components': [100, 150, 200, 250, 300],\n","    'rf__n_estimators': [100, 200, 400, 800, 1000],\n","    'rf__max_depth': [10, 20, 40],\n","    'rf__min_samples_split': [2, 5],\n","}\n","\n","# --- Define multiple scoring metrics ---\n","scoring = {\n","    'r2': make_scorer(r2_score),\n","    'rmse': make_scorer(lambda y_true, y_pred: mean_squared_error(y_true, y_pred, squared=False)),\n","    'mae': make_scorer(mean_absolute_error),\n","    'max_error': make_scorer(max_error)\n","}\n","\n","# --- Define outer and inner CV loops ---\n","outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n","inner_cv = KFold(n_splits=4, shuffle=True, random_state=42)\n","\n","# --- Storage for results ---\n","outer_results = []\n","\n","# --- Outer loop ---\n","for fold, (train_idx, test_idx) in enumerate(outer_cv.split(X)):\n","    print(f\"\\n=== Outer Fold {fold+1} ===\")\n","    X_train, X_test = X[train_idx], X[test_idx]\n","    y_train, y_test = y[train_idx], y[test_idx]\n","\n","    # --- Inner loop: hyperparameter tuning ---\n","    random_search = RandomizedSearchCV(\n","        estimator=pipeline,\n","        param_distributions=param_grid,\n","        n_iter=30,  # number of random combinations\n","        scoring='r2',\n","        cv=inner_cv,\n","        n_jobs=-1,\n","        random_state=42,\n","        verbose=1\n","    )\n","    random_search.fit(X_train, y_train)\n","\n","    # --- Get best model from inner CV ---\n","    best_model = random_search.best_estimator_\n","    print(\"Best params:\", random_search.best_params_)\n","\n","    # --- Evaluate on outer test set ---\n","    y_pred = best_model.predict(X_test)\n","    r2 = r2_score(y_test, y_pred)\n","    rmse = mean_squared_error(y_test, y_pred)\n","    mae = mean_absolute_error(y_test, y_pred)\n","    max_err = max_error(y_test, y_pred)\n","\n","    outer_results.append({'fold': fold+1, 'r2': r2, 'rmse': rmse, 'mae': mae, 'max_error': max_err})\n","\n","# --- Summary of outer results ---\n","print(\"\\n=== Nested Cross-Validation Results ===\")\n","for res in outer_results:\n","    print(f\"Fold {res['fold']}: R2={res['r2']:.4f}, RMSE={res['rmse']:.4f}, MAE={res['mae']:.4f}, MaxErr={res['max_error']:.4f}\")\n","\n","mean_r2 = np.mean([r['r2'] for r in outer_results])\n","\n"],"metadata":{"id":"Jig502n7jOVE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"M555BRjtP9k7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["DIMENSION 2"],"metadata":{"id":"NsOC6NodMLXb"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import KFold, RandomizedSearchCV\n","from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, max_error, make_scorer\n","\n","# --- Your data ---\n","X = X_sparse\n","y = y1\n","\n","# --- Define pipeline: SVD + Random Forest ---\n","pipeline = Pipeline([\n","    ('svd', TruncatedSVD(random_state=42)),\n","    ('rf', RandomForestRegressor(random_state=42))\n","])\n","\n","# --- Define hyperparameter space ---\n","param_grid = {\n","    'svd__n_components': [100, 150, 200, 250, 300],\n","    'rf__n_estimators': [100, 200, 400, 800, 1000],\n","    'rf__max_depth': [10, 20, 40],\n","    'rf__min_samples_split': [2, 5],\n","}\n","\n","# --- Define multiple scoring metrics ---\n","scoring = {\n","    'r2': make_scorer(r2_score),\n","    'rmse': make_scorer(lambda y_true, y_pred: mean_squared_error(y_true, y_pred)),\n","    'mae': make_scorer(mean_absolute_error),\n","    'max_error': make_scorer(max_error)\n","}\n","\n","# --- Define outer and inner CV loops ---\n","outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n","inner_cv = KFold(n_splits=4, shuffle=True, random_state=42)\n","\n","# --- Storage for results ---\n","outer_results = []\n","\n","# --- Outer loop ---\n","for fold, (train_idx, test_idx) in enumerate(outer_cv.split(X)):\n","    print(f\"\\n=== Outer Fold {fold+1} ===\")\n","    X_train, X_test = X[train_idx], X[test_idx]\n","    y_train, y_test = y[train_idx], y[test_idx]\n","\n","    # --- Inner loop: hyperparameter tuning ---\n","    random_search = RandomizedSearchCV(\n","        estimator=pipeline,\n","        param_distributions=param_grid,\n","        n_iter=30,  # number of random combinations\n","        scoring='r2',\n","        cv=inner_cv,\n","        n_jobs=-1,\n","        random_state=42,\n","        verbose=1\n","    )\n","    random_search.fit(X_train, y_train)\n","\n","    # --- Get best model from inner CV ---\n","    best_model = random_search.best_estimator_\n","    print(\"Best params:\", random_search.best_params_)\n","\n","    # --- Evaluate on outer test set ---\n","    y_pred = best_model.predict(X_test)\n","    r2 = r2_score(y_test, y_pred)\n","    rmse = mean_squared_error(y_test, y_pred)\n","    mae = mean_absolute_error(y_test, y_pred)\n","    max_err = max_error(y_test, y_pred)\n","\n","    outer_results.append({'fold': fold+1, 'r2': r2, 'rmse': rmse, 'mae': mae, 'max_error': max_err})\n"],"metadata":{"id":"PvtaxrdPMKC7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["FEATURE EXTRACTION - DIMENSION 1"],"metadata":{"id":"GXCCSOCCa_AQ"}},{"cell_type":"code","source":[],"metadata":{"id":"EhvyYSxRbAc0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["###SVD 100 -- DIMENSION 1 -- FEATURE EXTRACTION\n","import pandas as pd\n","import numpy as np\n","from sklearn.linear_model import Ridge, Lasso, SGDRegressor, LinearRegression\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.model_selection import KFold, RandomizedSearchCV\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, max_error\n","\n","##feature reduction\n","svd = TruncatedSVD(n_components=100, random_state = 42)\n","X = svd.fit_transform(X_sparse)\n","\n","feature_names = np.array(vectorizer.get_feature_names_out())\n","\n","models = {\n","    'Random Forest': RandomForestRegressor(\n","        n_estimators=1000,\n","        max_depth = 10,\n","        min_samples_split = 5,\n","        random_state = 42\n","        )\n","}\n","\n","for name, model in models.items():\n","    print(f\"Training {name}...\")\n","    model.fit(X, y)\n","\n","    #Feature importances for SVD components\n","    svd_importances = model.feature_importances_\n","\n","    #Map back to original TF-IDF features\n","    #components_.shape = (n_components, n_original_features)\n","    orig_importances = svd.components_.T @ svd_importances\n","\n","    # 3. Sort & get top features\n","    sorted_idx = np.argsort(orig_importances)\n","\n","    # Sort coefficients\n","    sorted_idx = np.argsort(orig_importances)\n","    top_neg_idx = sorted_idx[:20]   # 20 most negative\n","    top_pos_idx = sorted_idx[-20:]  # 20 most positive\n","\n","print(\"\\nTop Positive Features:\")\n","for i in top_pos_idx[::-1]:\n","   print(f\"{feature_names[i]}: {orig_importances[i]:.5f}\")\n","\n","print(\"\\nTop Negative Features:\")\n","for i in top_neg_idx:\n","    print(f\"{feature_names[i]}: {orig_importances[i]:.5f}\")\n","\n","\n"],"metadata":{"id":"c-Qc9OvoqkD3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["###SVD 100 -- DIMENSION 2 -- FEATURE EXTRACTION\n","import pandas as pd\n","import numpy as np\n","from sklearn.linear_model import Ridge, Lasso, SGDRegressor, LinearRegression\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.model_selection import KFold, RandomizedSearchCV\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, max_error\n","\n","##feature reduction\n","svd = TruncatedSVD(n_components=100, random_state = 42)\n","X = svd.fit_transform(X_sparse)\n","\n","feature_names = np.array(vectorizer.get_feature_names_out())\n","\n","models = {\n","    'Random Forest': RandomForestRegressor(\n","        n_estimators=1000,\n","        max_depth = 10,\n","        min_samples_split = 5,\n","        random_state = 42\n","        )\n","}\n","\n","for name, model in models.items():\n","    print(f\"Training {name}...\")\n","    model.fit(X, y1)\n","\n","    #Feature importances for SVD components\n","    svd_importances = model.feature_importances_\n","\n","    #Map back to original TF-IDF features\n","    #components_.shape = (n_components, n_original_features)\n","    orig_importances = svd.components_.T @ svd_importances\n","\n","    # 3. Sort & get top features\n","    sorted_idx = np.argsort(orig_importances)\n","\n","    # Sort coefficients\n","    sorted_idx = np.argsort(orig_importances)\n","    top_neg_idx = sorted_idx[:20]   # 20 most negative\n","    top_pos_idx = sorted_idx[-20:]  # 20 most positive\n","\n","print(\"\\nTop Positive Features:\")\n","for i in top_pos_idx[::-1]:\n","   print(f\"{feature_names[i]}: {orig_importances[i]:.5f}\")\n","\n","print(\"\\nTop Negative Features:\")\n","for i in top_neg_idx:\n","    print(f\"{feature_names[i]}: {orig_importances[i]:.5f}\")\n","\n"],"metadata":{"id":"HShSau8djUee"},"execution_count":null,"outputs":[]}]}
