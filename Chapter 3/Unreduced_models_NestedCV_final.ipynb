{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO/XQ5B6alJYsz/7323SC5y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"VfZNy7ZeGGAl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763418698985,"user_tz":300,"elapsed":1129,"user":{"displayName":"Michael Hammer","userId":"02778246655929855865"}},"outputId":"6be3167c-212d-4eee-b506-c2e866a97bdf"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["##import and format data\n","import pandas as pd\n","import numpy as np\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.decomposition import PCA\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","nltk.download('wordnet')\n","nltk.download('punkt')\n","\n","#Upload complete training dataset\n","all114 = pd.read_csv('training_data_114_final.csv', dtype=\"string\")\n","all2 = all114.astype({'nominate_dim1':'float', 'nominate_dim2': 'float'})\n","next114 = all2.dropna()\n","\n","###variable of NOMINATE values\n","ydim1 = next114.nominate_dim1\n","y1 = next114.nominate_dim2\n","\n","#Full training set text data\n","final114 = next114['speech']\n","\n","# OR Upload LEMMATIZED dataset -- DO NOT run this and the complete training dataset upload.  Choose one.\n","#all114 = pd.read_csv('lemmatized_output.csv', dtype=\"string\")\n","#docs = all114['speech']\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iozZpiKEG6DR"},"outputs":[],"source":["#upload and run the custom stopword list\n","from congress_stopwords import congress\n","\n"]},{"cell_type":"code","source":["####LEMMATIZE\n","doc = final114\n","\n","import spacy\n","\n","# Load the English language model\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# Function to lemmatize a single text entry\n","def lemmatize_text(text):\n","    text = str(text)\n","    if len(text) > 1_000_000:\n","        return \"TEXT TOO LONG\"\n","    doc = nlp(text)  # Ensure text is string\n","    return \" \".join([token.lemma_ for token in doc])\n","\n","# Apply to the whole column\n","docs = doc.apply(lemmatize_text)\n","\n","# Preview result\n","print(docs.head())"],"metadata":{"id":"yxo_83ri4Wy3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 4: Convert text to TF-IDF matrix and make it dense\n","\n","\n","vectorizer = TfidfVectorizer(stop_words=congress, min_df=5, max_df=0.5)\n","X_sparse = vectorizer.fit_transform(docs)\n","\n","\n","print(f\"TF-IDF shape before PCA: {X_sparse.shape}\")"],"metadata":{"id":"wAbN7VtLOkS7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["DIMENSION 1"],"metadata":{"id":"_XrOoVRWdg0U"}},{"cell_type":"code","source":["####CODE REMOVES RF AND DISPLAYS HYPERPARAMETERS\n","\n","from sklearn.linear_model import LinearRegression, Ridge, Lasso, SGDRegressor\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.model_selection import KFold, RandomizedSearchCV\n","from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, max_error\n","from scipy.stats import loguniform, uniform\n","import numpy as np\n","import pandas as pd\n","\n","# data\n","X = X_sparse\n","y = ydim1\n","\n","# === Define models and parameter search spaces ===\n","param_distributions = {\n","    'Linear Regression': {\n","        'model': LinearRegression(),\n","        'params': {}  # No hyperparameters to tune\n","    },\n","    'Ridge Regression': {\n","        'model': Ridge(),\n","        'params': {'alpha': loguniform(1e-3, 1e3)}\n","    },\n","    'Lasso Regression': {\n","        'model': Lasso(max_iter=5000),\n","        'params': {'alpha': loguniform(1e-4, 10)}\n","    },\n","    'SGD Regressor (squared_epsilon_insensitive)': {\n","        'model': SGDRegressor(\n","            loss='squared_epsilon_insensitive',\n","            max_iter=2000,\n","            random_state=42\n","        ),\n","        'params': {\n","            'alpha': loguniform(1e-5, 1e-1),\n","            'penalty': ['l1', 'l2', 'elasticnet'],\n","            'epsilon': uniform(0.001, 0.2),\n","            'learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive']\n","        }\n","    }\n","}\n","\n","# === Cross-validation===\n","outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n","inner_cv = KFold(n_splits=4, shuffle=True, random_state=42)\n","\n","# === Storage for results ===\n","nested_results = {}\n","\n","# === Nested Cross-Validation ===\n","for name, cfg in param_distributions.items():\n","    print(f\"\\n===== {name} =====\")\n","    outer_scores = []\n","    best_params_per_fold = []\n","\n","    for fold, (train_idx, test_idx) in enumerate(outer_cv.split(X)):\n","        print(f\"\\n--- Outer Fold {fold + 1} ---\")\n","\n","        X_train, X_test = X[train_idx], X[test_idx]\n","        y_train, y_test = y[train_idx], y[test_idx]\n","\n","        if cfg['params']:\n","            search = RandomizedSearchCV(\n","                estimator=cfg['model'],\n","                param_distributions=cfg['params'],\n","                n_iter=30,\n","                scoring='r2',\n","                cv=inner_cv,\n","                n_jobs=-1,\n","                random_state=42,\n","                verbose=0\n","            )\n","            search.fit(X_train, y_train)\n","            best_model = search.best_estimator_\n","            best_params = search.best_params_\n","            print(f\"Best params (fold {fold + 1}): {best_params}\")\n","        else:\n","            best_model = cfg['model']\n","            best_model.fit(X_train, y_train)\n","            best_params = {}\n","            print(f\"No hyperparameters to tune for {name}.\")\n","\n","        best_params_per_fold.append(best_params)\n","\n","        # Evaluate on outer test set\n","        y_pred = best_model.predict(X_test)\n","        r2 = r2_score(y_test, y_pred)\n","        rmse = mean_squared_error(y_test, y_pred)\n","        mae = mean_absolute_error(y_test, y_pred)\n","        max_err = max_error(y_test, y_pred)\n","\n","        outer_scores.append({\n","            'fold': fold + 1,\n","            'r2': r2,\n","            'rmse': rmse,\n","            'mae': mae,\n","            'max_error': max_err\n","        })\n","\n","        print(f\"Fold {fold + 1} R²: {r2:.4f} | RMSE: {rmse:.4f} | MAE: {mae:.4f} | MAX ERROR: {max_err:.4f}\")\n","\n","    # Aggregate outer CV results\n","    mean_r2 = np.mean([r['r2'] for r in outer_scores])\n","    mean_rmse = np.mean([r['rmse'] for r in outer_scores])\n","    mean_mae = np.mean([r['mae'] for r in outer_scores])\n","    mean_maxerr = np.mean([r['max_error'] for r in outer_scores])\n","\n","    # Summarize best params (if any)\n","    if best_params_per_fold and any(best_params_per_fold):\n","        params_df = pd.DataFrame(best_params_per_fold)\n","        mean_params = params_df.mode().iloc[0].to_dict()  # most common best params\n","    else:\n","        mean_params = {}\n","\n","    nested_results[name] = {\n","        'outer_scores': outer_scores,\n","        'mean_r2': mean_r2,\n","        'mean_rmse': mean_rmse,\n","        'mean_mae': mean_mae,\n","        'mean_max_error': mean_maxerr,\n","        'best_params_per_fold': best_params_per_fold,\n","        'mean_best_params': mean_params\n","    }\n","\n","    print(f\"\\nMean Outer R²: {mean_r2:.4f}\")\n","    print(f\"Mean Outer RMSE: {mean_rmse:.4f}\")\n","    print(f\"Mean Outer MAE: {mean_mae:.4f}\")\n","    print(f\"Mean Outer Max Error: {mean_maxerr:.4f}\")\n","    if mean_params:\n","        print(f\"Most common/best parameters across folds: {mean_params}\")\n","\n","# === Summary of all models ===\n","print(\"\\n===== Nested CV Summary =====\")\n","for name, res in nested_results.items():\n","    print(f\"\\n{name}:\")\n","    print(f\"  Mean R² = {res['mean_r2']:.4f}\")\n","    print(f\"  Mean RMSE = {res['mean_rmse']:.4f}\")\n","    print(f\"  Mean MAE = {res['mean_mae']:.4f}\")\n","    print(f\"  Mean Max Error = {res['mean_max_err']:.4f}\")\n","    if res['mean_best_params']:\n","        print(f\"  Representative Best Params: {res['mean_best_params']}\")\n"],"metadata":{"id":"_8AU1umQx49j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["DIMENSION 2"],"metadata":{"id":"o2kKITTydc4K"}},{"cell_type":"code","source":["###DIMENSION 2\n","\n","from sklearn.linear_model import LinearRegression, Ridge, Lasso, SGDRegressor\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.model_selection import KFold, RandomizedSearchCV\n","from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, max_error\n","from scipy.stats import loguniform, uniform\n","import numpy as np\n","import pandas as pd\n","\n","#  data\n","X = X_sparse\n","y = y1\n","\n","# === Define models and parameter search spaces ===\n","param_distributions = {\n","    'Linear Regression': {\n","        'model': LinearRegression(),\n","        'params': {}  # No hyperparameters to tune\n","    },\n","    'Ridge Regression': {\n","        'model': Ridge(),\n","        'params': {'alpha': loguniform(1e-3, 1e3)}\n","    },\n","    'Lasso Regression': {\n","        'model': Lasso(max_iter=5000),\n","        'params': {'alpha': loguniform(1e-4, 10)}\n","    },\n","    'SGD Regressor (squared_epsilon_insensitive)': {\n","        'model': SGDRegressor(\n","            loss='squared_epsilon_insensitive',\n","            max_iter=2000,\n","            random_state=42\n","        ),\n","        'params': {\n","            'alpha': loguniform(1e-5, 1e-1),\n","            'penalty': ['l1', 'l2', 'elasticnet'],\n","            'epsilon': uniform(0.001, 0.2),\n","            'learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive']\n","        }\n","    }\n","}\n","\n","# === Cross-validation ===\n","outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n","inner_cv = KFold(n_splits=4, shuffle=True, random_state=42)\n","\n","# === Storage for results ===\n","nested_results = {}\n","\n","# === Nested Cross-Validation ===\n","for name, cfg in param_distributions.items():\n","    print(f\"\\n===== {name} =====\")\n","    outer_scores = []\n","    best_params_per_fold = []\n","\n","    for fold, (train_idx, test_idx) in enumerate(outer_cv.split(X)):\n","        print(f\"\\n--- Outer Fold {fold + 1} ---\")\n","\n","        X_train, X_test = X[train_idx], X[test_idx]\n","        y_train, y_test = y[train_idx], y[test_idx]\n","\n","        if cfg['params']:\n","            search = RandomizedSearchCV(\n","                estimator=cfg['model'],\n","                param_distributions=cfg['params'],\n","                n_iter=30,\n","                scoring='r2',\n","                cv=inner_cv,\n","                n_jobs=-1,\n","                random_state=42,\n","                verbose=0\n","            )\n","            search.fit(X_train, y_train)\n","            best_model = search.best_estimator_\n","            best_params = search.best_params_\n","            print(f\"Best params (fold {fold + 1}): {best_params}\")\n","        else:\n","            best_model = cfg['model']\n","            best_model.fit(X_train, y_train)\n","            best_params = {}\n","            print(f\"No hyperparameters to tune for {name}.\")\n","\n","        best_params_per_fold.append(best_params)\n","\n","        # Evaluate on outer test set\n","        y_pred = best_model.predict(X_test)\n","        r2 = r2_score(y_test, y_pred)\n","        rmse = mean_squared_error(y_test, y_pred)\n","        mae = mean_absolute_error(y_test, y_pred)\n","        max_err = max_error(y_test, y_pred)\n","\n","        outer_scores.append({\n","            'fold': fold + 1,\n","            'r2': r2,\n","            'rmse': rmse,\n","            'mae': mae,\n","            'max_error': max_err\n","        })\n","\n","        print(f\"Fold {fold + 1} R²: {r2:.4f} | RMSE: {rmse:.4f} | MAE: {mae:.4f} | Max Err: {max_err:.4f}\")\n","\n","    # Aggregate outer CV results\n","    mean_r2 = np.mean([r['r2'] for r in outer_scores])\n","    mean_rmse = np.mean([r['rmse'] for r in outer_scores])\n","    mean_mae = np.mean([r['mae'] for r in outer_scores])\n","    mean_maxerr = np.mean([r['max_error'] for r in outer_scores])\n","\n","    # Summarize best params (if any)\n","    if best_params_per_fold and any(best_params_per_fold):\n","        params_df = pd.DataFrame(best_params_per_fold)\n","        mean_params = params_df.mode().iloc[0].to_dict()  # most common best params\n","    else:\n","        mean_params = {}\n","\n","    nested_results[name] = {\n","        'outer_scores': outer_scores,\n","        'mean_r2': mean_r2,\n","        'mean_rmse': mean_rmse,\n","        'mean_mae': mean_mae,\n","        'mean_max_error': mean_maxerr,\n","        'best_params_per_fold': best_params_per_fold,\n","        'mean_best_params': mean_params\n","    }\n","\n","    print(f\"\\nMean Outer R²: {mean_r2:.4f}\")\n","    print(f\"Mean Outer RMSE: {mean_rmse:.4f}\")\n","    print(f\"Mean Outer MAE: {mean_mae:.4f}\")\n","    print(f\"Mean Outer Max Error: {mean_maxerr:.4f}\")\n","    if mean_params:\n","        print(f\"Most common/best parameters across folds: {mean_params}\")\n","\n","# === Summary of all models ===\n","print(\"\\n===== Nested CV Summary =====\")\n","for name, res in nested_results.items():\n","    print(f\"\\n{name}:\")\n","    print(f\"  Mean R² = {res['mean_r2']:.4f}\")\n","    print(f\"  Mean RMSE = {res['mean_rmse']:.4f}\")\n","    print(f\"  Mean MAE = {res['mean_mae']:.4f}\")\n","    if res['mean_best_params']:\n","        print(f\"  Representative Best Params: {res['mean_best_params']}\")\n"],"metadata":{"id":"2dCuajdQx9VN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["FEATURE EXTRACTION - DIMENSION 1"],"metadata":{"id":"YOtmkPinI2n9"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.linear_model import Ridge, Lasso, SGDRegressor, LinearRegression\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.model_selection import KFold, RandomizedSearchCV\n","from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, max_error\n","\n","X = X_sparse\n","y = ydim1\n","\n","\n","feature_names = np.array(vectorizer.get_feature_names_out())\n","\n","models = {\n","    'Linear Regression': LinearRegression(),\n","    'Ridge Regression': Ridge(alpha=0.05589),\n","    'Lasso Regression': Lasso(max_iter=5000, alpha=0.000498),\n","    'SGD Regressor (squared_epsilon_insensitive)': SGDRegressor(\n","        loss='squared_epsilon_insensitive',\n","        max_iter=2000,\n","        random_state=42,\n","        alpha=0.000366,\n","        penalty='l2',\n","        epsilon=0.0374,\n","        learning_rate='adaptive'\n","    )\n","}\n","\n","for name, model in models.items():\n","    print(f\"Training {name}...\")\n","    model.fit(X, y)\n","    coefs = model.coef_\n","\n","    # Sort coefficients\n","    sorted_idx = np.argsort(coefs)\n","    top_neg_idx = sorted_idx[:20]   # 5 most negative\n","    top_pos_idx = sorted_idx[-20:]  # 5 most positive\n","\n","    print(f\"\\n=== {name} ===\")\n","    print(\"Top positive words (increase Dim 1 score):\")\n","    for i in top_pos_idx[::-1]:\n","        print(f\"  {feature_names[i]:<15} {coefs[i]:.4f}\")\n","\n","    print(\"Top negative words (decrease Dim 1 score):\")\n","    for i in top_neg_idx:\n","        print(f\"  {feature_names[i]:<15} {coefs[i]:.4f}\")\n","\n","\n","\n"],"metadata":{"id":"RL8KxCmTgYF9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["FEATURE EXTRACTION - DIMENSION 2"],"metadata":{"id":"EhRvtAIoI__Q"}},{"cell_type":"code","source":["####FEATURE EXTRACTION -- DIMENSION 2\n","\n","import pandas as pd\n","import numpy as np\n","from sklearn.linear_model import Ridge, Lasso, SGDRegressor, LinearRegression\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.model_selection import KFold, RandomizedSearchCV\n","from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, max_error\n","\n","X = X_sparse\n","y = y1\n","\n","feature_names = np.array(vectorizer.get_feature_names_out())\n","\n","models = {\n","    'Linear Regression': LinearRegression(),\n","    'Ridge Regression': Ridge(alpha=0.05589),\n","    'Lasso Regression': Lasso(max_iter=5000, alpha=0.000498),\n","    'SGD Regressor (squared_epsilon_insensitive)': SGDRegressor(\n","        loss='squared_epsilon_insensitive',\n","        max_iter=2000,\n","        random_state=42,\n","        alpha=0.000366,\n","        penalty='l2',\n","        epsilon=0.0374,\n","        learning_rate='adaptive'\n","    )\n","}\n","\n","for name, model in models.items():\n","    print(f\"Training {name}...\")\n","    model.fit(X, y1)\n","    coefs = model.coef_\n","\n","    # Sort coefficients\n","    sorted_idx = np.argsort(coefs)\n","    top_neg_idx = sorted_idx[:20]   # 5 most negative\n","    top_pos_idx = sorted_idx[-20:]  # 5 most positive\n","\n","    print(f\"\\n=== {name} ===\")\n","    print(\"Top positive words (increase Dim 1 score):\")\n","    for i in top_pos_idx[::-1]:\n","        print(f\"  {feature_names[i]:<15} {coefs[i]:.4f}\")\n","\n","    print(\"Top negative words (decrease Dim 1 score):\")\n","    for i in top_neg_idx:\n","        print(f\"  {feature_names[i]:<15} {coefs[i]:.4f}\")\n","\n","\n","\n"],"metadata":{"id":"r3m2iiosmEpK"},"execution_count":null,"outputs":[]}]}
