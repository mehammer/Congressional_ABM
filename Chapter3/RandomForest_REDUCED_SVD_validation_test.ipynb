{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPkFu1uA219HJAnf1c2WJNl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"VfZNy7ZeGGAl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761505957523,"user_tz":240,"elapsed":5539,"user":{"displayName":"Michael Hammer","userId":"02778246655929855865"}},"outputId":"ec81bb6d-f106-46bc-e825-5a6a0057e290"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}],"source":["##import and format data\n","import pandas as pd\n","import numpy as np\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.decomposition import PCA\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","nltk.download('wordnet')\n","nltk.download('punkt')\n","\n","#Upload complete training dataset\n","all114 = pd.read_csv('training_data_114_final.csv', dtype=\"string\")\n","all2 = all114.astype({'nominate_dim1':'float', 'nominate_dim2': 'float'})\n","next114 = all2.dropna()\n","\n","###variable of NOMINATE values\n","y = next114.nominate_dim1\n","y1 = next114.nominate_dim2\n","\n","# OR Upload LEMMATIZED dataset -- DO NOT run this and the complete training dataset upload.  Choose one.\n","all114 = pd.read_csv('lemmatized_output.csv', dtype=\"string\")\n","docs = all114['speech']\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iozZpiKEG6DR"},"outputs":[],"source":["#upload and run the custom stopword list\n","from congress_stopwords import congress\n","\n"]},{"cell_type":"code","source":["####LEMMATIZE\n","doc = final114\n","\n","import spacy\n","\n","# Load the English language model\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# Function to lemmatize a single text entry\n","def lemmatize_text(text):\n","    text = str(text)\n","    if len(text) > 1_000_000:\n","        return \"TEXT TOO LONG\"\n","    doc = nlp(text)  # Ensure text is string\n","    return \" \".join([token.lemma_ for token in doc])\n","\n","# Apply to the whole column\n","docs = doc.apply(lemmatize_text)\n","\n","# Preview result\n","print(docs.head())"],"metadata":{"id":"yxo_83ri4Wy3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 4: Convert text to TF-IDF matrix and make it dense\n","\n","\n","vectorizer = TfidfVectorizer(stop_words=congress, min_df=5, max_df=0.5)\n","X_sparse = vectorizer.fit_transform(docs)\n","#X_dense = X_sparse.toarray()  # Convert to dense format for PCA\n","\n","print(f\"TF-IDF shape before PCA: {X_sparse.shape}\")"],"metadata":{"id":"wAbN7VtLOkS7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.decomposition import TruncatedSVD\n","\n","#SVD = 100\n","n_components = 100\n","svd100 = TruncatedSVD(n_components=n_components)\n","X100 = svd100.fit_transform(X_sparse)\n","\n","print(f\"PCA shape after reduction: {X100.shape}\") # second dimension should equal n_components"],"metadata":{"id":"rtQ98kJ1fNRW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.decomposition import TruncatedSVD\n","\n","#SVD = 250\n","n_components = 250\n","svd250 = TruncatedSVD(n_components=n_components)\n","X250 = svd250.fit_transform(X_sparse)\n","\n","print(f\"PCA shape after reduction: {X250.shape}\") # second dimension should equal n_components"],"metadata":{"id":"gwyNBJhvf5j5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["VALIDATION"],"metadata":{"id":"XxHmxexVClXU"}},{"cell_type":"code","source":["###116th CONGRESSIONAL RECORD speeches\n","all116 = pd.read_csv('116incCR1.txt', dtype=\"string\", sep = ',')\n","all2 = all116.astype({'score':'float','dim2':'float'})\n","cr116_1 = all2.dropna()\n"],"metadata":{"id":"ErOfJwyXdccp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["val_speech = cr116_1['text']\n"],"metadata":{"id":"BlLB3GZIjEjj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["doc = val_speech\n","\n","import spacy\n","\n","# Load the English language model\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# Function to lemmatize a single text entry\n","def lemmatize_text(text):\n","    text = str(text)\n","    if len(text) > 1_000_000:\n","        return \"TEXT TOO LONG\"\n","    doc = nlp(text)  # Ensure text is string\n","    return \" \".join([token.lemma_ for token in doc])\n","\n","# Apply to the whole column\n","val_speeches = doc.apply(lemmatize_text)\n","\n","# Preview result\n","print(val_speeches.head())"],"metadata":{"id":"LjzYp3EBNZmd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(val_speeches)"],"metadata":{"id":"PO5nRdTYjFqR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_val = vectorizer.transform(val_speeches)\n","y_val = cr116_1.score\n","y_val1 = cr116_1.dim2"],"metadata":{"id":"8v2svcd8g4qj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_val.shape"],"metadata":{"id":"imu5s2kejNb5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#SVD = 250\n","#n_components = 250\n","#svd250 = TruncatedSVD(n_components=n_components)\n","X_val_svd = svd250.transform(X_val)\n","\n","print(f\"shape after reduction: {X_val_svd.shape}\") # second dimension should equal n_components"],"metadata":{"id":"qHY9UBhxiStt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#SVD = 100\n","#n_components = 100\n","#svd250 = TruncatedSVD(n_components=n_components)\n","X_val_svd100 = svd100.transform(X_val)\n","\n","print(f\"shape after reduction: {X_val_svd100.shape}\") # second dimension should equal n_components"],"metadata":{"id":"UcQIi49hOAgd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.pipeline import Pipeline\n","from sklearn.metrics import make_scorer, r2_score, mean_squared_error, mean_absolute_error, max_error\n","\n","###VALIDATION DATA 1 --DIMENSION 1-- 116 CR -- SVD 250\n","model = RandomForestRegressor(n_estimators=1000, max_depth = 10, min_samples_split = 5)\n","model.fit(X250, y)\n","\n","y_val_pred = model.predict(X_val_svd)\n","\n","\n","r2_val = r2_score(y_val, y_val_pred)\n","rmse_val = np.sqrt(mean_squared_error(y_val, y_val_pred))\n","mae_val = mean_absolute_error(y_val, y_val_pred)\n","max_err_val = max_error(y_val, y_val_pred)\n","\n","# Print the evaluation metrics for the validation data\n","print(\"Validation Set Evaluation:\")\n","print(f\"R-squared: {r2_val}\")\n","print(f\"RMSE: {rmse_val}\")\n","print(f\"MAE: {mae_val}\")\n","print(f\"Max Error: {max_err_val}\")\n","\n"],"metadata":{"id":"iZmGBGR8rcn2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["###export trained model for SVD = 250\n","import joblib\n","\n","joblib.dump(model,'svd250_full.pkl')"],"metadata":{"id":"0g5RFG7hlGIq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#### CR116 - SVD 100\n","import numpy as np\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.pipeline import Pipeline\n","from sklearn.metrics import make_scorer, r2_score, mean_squared_error, mean_absolute_error, max_error\n","\n","###VALIDATION DATA 1 --DIMENSION 1-- 116 CR -- SVD 100\n","model = RandomForestRegressor(n_estimators=1000, max_depth = 10, min_samples_split = 5)\n","model.fit(X100, y)\n","\n","y_val_pred = model.predict(X_val_svd100)\n","\n","\n","r2_val = r2_score(y_val, y_val_pred)\n","rmse_val = np.sqrt(mean_squared_error(y_val, y_val_pred))\n","mae_val = mean_absolute_error(y_val, y_val_pred)\n","max_err_val = max_error(y_val, y_val_pred)\n","\n","# Print the evaluation metrics for the validation data\n","print(\"Validation Set Evaluation:\")\n","print(f\"R-squared: {r2_val}\")\n","print(f\"RMSE: {rmse_val}\")\n","print(f\"MAE: {mae_val}\")\n","print(f\"Max Error: {max_err_val}\")\n","\n"],"metadata":{"id":"tm1NFq8lQ4hm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["PREDICTION OF CANDIDATE VALUES"],"metadata":{"id":"urgxFS3hzDRV"}},{"cell_type":"code","source":["all116 = pd.read_csv('116cand.csv', dtype=\"string\", sep = ',')\n","cand116 = all116.dropna()\n","cand116.shape"],"metadata":{"id":"7vMkIuIdhiVw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cand_speech = cand116['text']"],"metadata":{"id":"Xr8EwGDuzBXg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["doc = cand_speech\n","\n","import spacy\n","\n","# Load the English language model\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# Function to lemmatize a single text entry\n","def lemmatize_text(text):\n","    text = str(text)\n","    if len(text) > 1_000_000:\n","        return \"TEXT TOO LONG\"\n","    doc = nlp(text)  # Ensure text is string\n","    return \" \".join([token.lemma_ for token in doc])\n","\n","# Apply to the whole column\n","cand_speeches = doc.apply(lemmatize_text)\n","\n","# Preview result\n","print(cand_speeches.head())"],"metadata":{"id":"f-wx_P6nQagX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cand_test = vectorizer.transform(cand_speeches)\n"],"metadata":{"id":"ACYpWL7YIcKc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#SVD = 250\n","#n_components = 250\n","#svd250 = TruncatedSVD(n_components=n_components)\n","X_cand_svd250 = svd250.transform(cand_test)\n","\n","print(f\"shape after reduction: {X_cand_svd250.shape}\") # second dimension should equal n_components"],"metadata":{"id":"ZZIQUzz8iQI2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["###TEST -- DIMENSION 1 -- CANDIDATE PREDICTIONS\n","RandomForestRegressor(n_estimators=1000, max_depth = 10, min_samples_split = 5)\n","model.fit(X250,y)\n","\n","import joblib\n","model = joblib.load('svd250_full.pkl')\n","y_cand_pred = model.predict(X_cand_svd250)"],"metadata":{"id":"tJ6TADmBBMIJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(y_cand_pred)"],"metadata":{"id":"XPL5gq39mu6R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#SVD = 100\n","#n_components = 100\n","#svd100 = TruncatedSVD(n_components=n_components)\n","X_cand_svd100 = svd100.transform(cand_test)\n","\n","print(f\"shape after reduction: {X_cand_svd100.shape}\") # second dimension should equal n_components"],"metadata":{"id":"Y6CTWzs8RsxT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["###TEST -- DIMENSION 1 -- CANDIDATE PREDICTIONS\n","RandomForestRegressor(n_estimators=1000, max_depth = 10, min_samples_split = 5)\n","model.fit(X100,y)\n","\n","import joblib\n","model = joblib.load('svd100_full.pkl')\n","y_cand_pred = model.predict(X_cand_svd100)"],"metadata":{"id":"wBGyHCAdm-AA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(y_cand_pred)"],"metadata":{"id":"8QM_XbKZnFWu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["DIMENSION 2"],"metadata":{"id":"FVnXLZONhb97"}},{"cell_type":"code","source":["###VALIDATION DATA 1 --DIMENSION 2-- 116 CR -- SVD250\n","model = RandomForestRegressor(n_estimators=1000, max_depth = 10, min_samples_split = 5)\n","model.fit(X250, y1)\n","\n","y_val_pred1 = model.predict(X_val_svd)\n","\n","\n","r2_val = r2_score(y_val1, y_val_pred1)\n","rmse_val = np.sqrt(mean_squared_error(y_val1, y_val_pred1))\n","mae_val = mean_absolute_error(y_val1, y_val_pred1)\n","max_err_val = max_error(y_val1, y_val_pred1)\n","\n","# Print the evaluation metrics for the validation data\n","print(\"Validation Set Evaluation:\")\n","print(f\"R-squared: {r2_val}\")\n","print(f\"RMSE: {rmse_val}\")\n","print(f\"MAE: {mae_val}\")\n","print(f\"Max Error: {max_err_val}\")\n","\n"],"metadata":{"id":"DPQ2jiUucDVX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.pipeline import Pipeline\n","from sklearn.metrics import make_scorer, r2_score, mean_squared_error, mean_absolute_error, max_error\n","\n","###VALIDATION DATA 1 --DIMENSION 2-- 116 CR -- SVD100\n","model = RandomForestRegressor(n_estimators=800, max_depth = 10, min_samples_split = 2)\n","model.fit(X100, y1)\n","\n","y_val_pred1 = model.predict(X_val_svd100)\n","\n","\n","r2_val = r2_score(y_val1, y_val_pred1)\n","rmse_val = np.sqrt(mean_squared_error(y_val1, y_val_pred1))\n","mae_val = mean_absolute_error(y_val1, y_val_pred1)\n","max_err_val = max_error(y_val1, y_val_pred1)\n","\n","# Print the evaluation metrics for the validation data\n","print(\"Validation Set Evaluation:\")\n","print(f\"R-squared: {r2_val}\")\n","print(f\"RMSE: {rmse_val}\")\n","print(f\"MAE: {mae_val}\")\n","print(f\"Max Error: {max_err_val}\")\n","\n"],"metadata":{"id":"6gEdW_Umh--z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["###TEST -- DIMENSION 2 -- CANDIDATE PREDICTIONS\n","RandomForestRegressor(n_estimators=1000, max_depth = 10, min_samples_split = 5)\n","model.fit(X250,y1)\n","\n","import joblib\n","joblib.dump(model,'svd250_full.pkl')\n","\n","\n","y_cand_pred = model.predict(X_cand_svd250)"],"metadata":{"id":"vXttIXBoiKZT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["###TEST -- DIMENSION 2 -- CANDIDATE PREDICTIONS -- SVD 100\n","RandomForestRegressor(n_estimators=1000, max_depth = 10, min_samples_split = 5)\n","model.fit(X100,y1)\n","\n","#import joblib\n","#joblib.dump(model,'svd100_full.pkl')\n","\n","y_cand_pred = model.predict(X_cand_svd100)"],"metadata":{"id":"Vhpg69kzb9xp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(y_cand_pred)"],"metadata":{"id":"2cmXwVFR9cwi"},"execution_count":null,"outputs":[]}]}