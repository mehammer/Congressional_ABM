{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPokcAmWo/rZMMnv/NfqK5v"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["11/16/25: This is an update of the notebook previously labeled \"Unreduced_models_NestedCV_withMAXERROR.\"  Includes the following:\n","\n","1. Text feature extraction\n","2. 2nd test of Dimension 2 with geographic area"],"metadata":{"id":"qmHWy9TzFvJU"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"VfZNy7ZeGGAl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763418698985,"user_tz":300,"elapsed":1129,"user":{"displayName":"Michael Hammer","userId":"02778246655929855865"}},"outputId":"6be3167c-212d-4eee-b506-c2e866a97bdf"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["##import and format data\n","import pandas as pd\n","import numpy as np\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.decomposition import PCA\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","nltk.download('wordnet')\n","nltk.download('punkt')\n","\n","#Upload complete training dataset\n","all114 = pd.read_csv('training_data_114_final.csv', dtype=\"string\")\n","all2 = all114.astype({'nominate_dim1':'float', 'nominate_dim2': 'float'})\n","next114 = all2.dropna()\n","\n","###variable of NOMINATE values\n","ydim1 = next114.nominate_dim1\n","y1 = next114.nominate_dim2\n","\n","#Full training set text data\n","final114 = next114['speech']\n","\n","# OR Upload LEMMATIZED dataset -- DO NOT run this and the complete training dataset upload.  Choose one.\n","#all114 = pd.read_csv('lemmatized_output.csv', dtype=\"string\")\n","#docs = all114['speech']\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"iozZpiKEG6DR","executionInfo":{"status":"ok","timestamp":1763418682153,"user_tz":300,"elapsed":41,"user":{"displayName":"Michael Hammer","userId":"02778246655929855865"}}},"outputs":[],"source":["#upload and run the custom stopword list\n","from congress_stopwords import congress\n","\n"]},{"cell_type":"code","source":["####LEMMATIZE\n","doc = final114\n","\n","import spacy\n","\n","# Load the English language model\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# Function to lemmatize a single text entry\n","def lemmatize_text(text):\n","    text = str(text)\n","    if len(text) > 1_000_000:\n","        return \"TEXT TOO LONG\"\n","    doc = nlp(text)  # Ensure text is string\n","    return \" \".join([token.lemma_ for token in doc])\n","\n","# Apply to the whole column\n","docs = doc.apply(lemmatize_text)\n","\n","# Preview result\n","print(docs.head())"],"metadata":{"id":"yxo_83ri4Wy3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763420288519,"user_tz":300,"elapsed":1578801,"user":{"displayName":"Michael Hammer","userId":"02778246655929855865"}},"outputId":"92afb631-89f0-4c9c-a0e5-2109d2fbd14d"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["0    Mr. Speaker . nearly 160 million Americans rec...\n","1    let I thank my colleague for yielding . and le...\n","2    but neither the United States nor any state sh...\n","3    Mr. Speaker . the Energy and Commerce Committe...\n","4    Mr. Speaker . I rise today to ask my colleague...\n","Name: speech, dtype: object\n"]}]},{"cell_type":"code","source":["# Step 4: Convert text to TF-IDF matrix and make it dense\n","\n","\n","vectorizer = TfidfVectorizer(stop_words=congress, min_df=5, max_df=0.5)\n","X_sparse = vectorizer.fit_transform(docs)\n","\n","\n","print(f\"TF-IDF shape before PCA: {X_sparse.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wAbN7VtLOkS7","executionInfo":{"status":"ok","timestamp":1763420585973,"user_tz":300,"elapsed":13653,"user":{"displayName":"Michael Hammer","userId":"02778246655929855865"}},"outputId":"95daf15c-1bb2-4f78-93b8-cf7d5204ff98"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["TF-IDF shape before PCA: (438, 14539)\n"]}]},{"cell_type":"markdown","source":["DIMENSION 1"],"metadata":{"id":"_XrOoVRWdg0U"}},{"cell_type":"code","source":["####CODE REMOVES RF AND DISPLAYS HYPERPARAMETERS\n","\n","from sklearn.linear_model import LinearRegression, Ridge, Lasso, SGDRegressor\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.model_selection import KFold, RandomizedSearchCV\n","from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, max_error\n","from scipy.stats import loguniform, uniform\n","import numpy as np\n","import pandas as pd\n","\n","# === Example data ===\n","# Replace with your actual data\n","X = X_sparse\n","y = ydim1\n","\n","# === Define models and parameter search spaces ===\n","param_distributions = {\n","    'Linear Regression': {\n","        'model': LinearRegression(),\n","        'params': {}  # No hyperparameters to tune\n","    },\n","    'Ridge Regression': {\n","        'model': Ridge(),\n","        'params': {'alpha': loguniform(1e-3, 1e3)}\n","    },\n","    'Lasso Regression': {\n","        'model': Lasso(max_iter=5000),\n","        'params': {'alpha': loguniform(1e-4, 10)}\n","    },\n","    'SGD Regressor (squared_epsilon_insensitive)': {\n","        'model': SGDRegressor(\n","            loss='squared_epsilon_insensitive',\n","            max_iter=2000,\n","            random_state=42\n","        ),\n","        'params': {\n","            'alpha': loguniform(1e-5, 1e-1),\n","            'penalty': ['l1', 'l2', 'elasticnet'],\n","            'epsilon': uniform(0.001, 0.2),\n","            'learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive']\n","        }\n","    }\n","}\n","\n","# === Cross-validation strategies ===\n","outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n","inner_cv = KFold(n_splits=4, shuffle=True, random_state=42)\n","\n","# === Storage for results ===\n","nested_results = {}\n","\n","# === Nested Cross-Validation ===\n","for name, cfg in param_distributions.items():\n","    print(f\"\\n===== {name} =====\")\n","    outer_scores = []\n","    best_params_per_fold = []\n","\n","    for fold, (train_idx, test_idx) in enumerate(outer_cv.split(X)):\n","        print(f\"\\n--- Outer Fold {fold + 1} ---\")\n","\n","        X_train, X_test = X[train_idx], X[test_idx]\n","        y_train, y_test = y[train_idx], y[test_idx]\n","\n","        if cfg['params']:\n","            search = RandomizedSearchCV(\n","                estimator=cfg['model'],\n","                param_distributions=cfg['params'],\n","                n_iter=30,\n","                scoring='r2',\n","                cv=inner_cv,\n","                n_jobs=-1,\n","                random_state=42,\n","                verbose=0\n","            )\n","            search.fit(X_train, y_train)\n","            best_model = search.best_estimator_\n","            best_params = search.best_params_\n","            print(f\"Best params (fold {fold + 1}): {best_params}\")\n","        else:\n","            best_model = cfg['model']\n","            best_model.fit(X_train, y_train)\n","            best_params = {}\n","            print(f\"No hyperparameters to tune for {name}.\")\n","\n","        best_params_per_fold.append(best_params)\n","\n","        # Evaluate on outer test set\n","        y_pred = best_model.predict(X_test)\n","        r2 = r2_score(y_test, y_pred)\n","        rmse = mean_squared_error(y_test, y_pred)\n","        mae = mean_absolute_error(y_test, y_pred)\n","        max_err = max_error(y_test, y_pred)\n","\n","        outer_scores.append({\n","            'fold': fold + 1,\n","            'r2': r2,\n","            'rmse': rmse,\n","            'mae': mae,\n","            'max_error': max_err\n","        })\n","\n","        print(f\"Fold {fold + 1} R²: {r2:.4f} | RMSE: {rmse:.4f} | MAE: {mae:.4f} | MAX ERROR: {max_err:.4f}\")\n","\n","    # Aggregate outer CV results\n","    mean_r2 = np.mean([r['r2'] for r in outer_scores])\n","    mean_rmse = np.mean([r['rmse'] for r in outer_scores])\n","    mean_mae = np.mean([r['mae'] for r in outer_scores])\n","    mean_maxerr = np.mean([r['max_error'] for r in outer_scores])\n","\n","    # Summarize best params (if any)\n","    if best_params_per_fold and any(best_params_per_fold):\n","        params_df = pd.DataFrame(best_params_per_fold)\n","        mean_params = params_df.mode().iloc[0].to_dict()  # most common best params\n","    else:\n","        mean_params = {}\n","\n","    nested_results[name] = {\n","        'outer_scores': outer_scores,\n","        'mean_r2': mean_r2,\n","        'mean_rmse': mean_rmse,\n","        'mean_mae': mean_mae,\n","        'mean_max_error': mean_maxerr,\n","        'best_params_per_fold': best_params_per_fold,\n","        'mean_best_params': mean_params\n","    }\n","\n","    print(f\"\\nMean Outer R²: {mean_r2:.4f}\")\n","    print(f\"Mean Outer RMSE: {mean_rmse:.4f}\")\n","    print(f\"Mean Outer MAE: {mean_mae:.4f}\")\n","    print(f\"Mean Outer Max Error: {mean_maxerr:.4f}\")\n","    if mean_params:\n","        print(f\"Most common/best parameters across folds: {mean_params}\")\n","\n","# === Summary of all models ===\n","print(\"\\n===== Nested CV Summary =====\")\n","for name, res in nested_results.items():\n","    print(f\"\\n{name}:\")\n","    print(f\"  Mean R² = {res['mean_r2']:.4f}\")\n","    print(f\"  Mean RMSE = {res['mean_rmse']:.4f}\")\n","    print(f\"  Mean MAE = {res['mean_mae']:.4f}\")\n","    print(f\"  Mean Max Error = {res['mean_max_err']:.4f}\")\n","    if res['mean_best_params']:\n","        print(f\"  Representative Best Params: {res['mean_best_params']}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"_8AU1umQx49j","executionInfo":{"status":"error","timestamp":1763420830704,"user_tz":300,"elapsed":182188,"user":{"displayName":"Michael Hammer","userId":"02778246655929855865"}},"outputId":"7da508c7-bc84-412e-9cd9-2d8f3eb88bfb"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","===== Linear Regression =====\n","\n","--- Outer Fold 1 ---\n","No hyperparameters to tune for Linear Regression.\n","Fold 1 R²: 0.5239 | RMSE: 0.0981 | MAE: 0.2446 | MAX ERROR: 0.7923\n","\n","--- Outer Fold 2 ---\n","No hyperparameters to tune for Linear Regression.\n","Fold 2 R²: 0.1287 | RMSE: 0.1682 | MAE: 0.3111 | MAX ERROR: 1.3200\n","\n","--- Outer Fold 3 ---\n","No hyperparameters to tune for Linear Regression.\n","Fold 3 R²: 0.4194 | RMSE: 0.1299 | MAE: 0.2850 | MAX ERROR: 1.3200\n","\n","--- Outer Fold 4 ---\n","No hyperparameters to tune for Linear Regression.\n","Fold 4 R²: 0.5372 | RMSE: 0.0910 | MAE: 0.2598 | MAX ERROR: 0.6887\n","\n","--- Outer Fold 5 ---\n","No hyperparameters to tune for Linear Regression.\n","Fold 5 R²: 0.4696 | RMSE: 0.1036 | MAE: 0.2645 | MAX ERROR: 1.0330\n","\n","Mean Outer R²: 0.4157\n","Mean Outer RMSE: 0.1182\n","Mean Outer MAE: 0.2730\n","Mean Outer Max Error: 1.0308\n","\n","===== Ridge Regression =====\n","\n","--- Outer Fold 1 ---\n","Best params (fold 1): {'alpha': np.float64(0.055895242052179224)}\n","Fold 1 R²: 0.5313 | RMSE: 0.0966 | MAE: 0.2474 | MAX ERROR: 0.7555\n","\n","--- Outer Fold 2 ---\n","Best params (fold 2): {'alpha': np.float64(0.1578232781079558)}\n","Fold 2 R²: 0.6036 | RMSE: 0.0765 | MAE: 0.2271 | MAX ERROR: 0.6153\n","\n","--- Outer Fold 3 ---\n","Best params (fold 3): {'alpha': np.float64(0.06690421166498801)}\n","Fold 3 R²: 0.5532 | RMSE: 0.1000 | MAE: 0.2662 | MAX ERROR: 0.9647\n","\n","--- Outer Fold 4 ---\n","Best params (fold 4): {'alpha': np.float64(0.055895242052179224)}\n","Fold 4 R²: 0.5467 | RMSE: 0.0891 | MAE: 0.2597 | MAX ERROR: 0.7606\n","\n","--- Outer Fold 5 ---\n","Best params (fold 5): {'alpha': np.float64(0.055895242052179224)}\n","Fold 5 R²: 0.4812 | RMSE: 0.1013 | MAE: 0.2636 | MAX ERROR: 0.9854\n","\n","Mean Outer R²: 0.5432\n","Mean Outer RMSE: 0.0927\n","Mean Outer MAE: 0.2528\n","Mean Outer Max Error: 0.8163\n","Most common/best parameters across folds: {'alpha': 0.055895242052179224}\n","\n","===== Lasso Regression =====\n","\n","--- Outer Fold 1 ---\n","Best params (fold 1): {'alpha': np.float64(0.0004982752357076451)}\n","Fold 1 R²: 0.4318 | RMSE: 0.1171 | MAE: 0.2936 | MAX ERROR: 0.7987\n","\n","--- Outer Fold 2 ---\n","Best params (fold 2): {'alpha': np.float64(0.0004982752357076451)}\n","Fold 2 R²: 0.5158 | RMSE: 0.0935 | MAE: 0.2598 | MAX ERROR: 0.8028\n","\n","--- Outer Fold 3 ---\n","Best params (fold 3): {'alpha': np.float64(0.00019517224641449495)}\n","Fold 3 R²: 0.4578 | RMSE: 0.1213 | MAE: 0.2909 | MAX ERROR: 0.9754\n","\n","--- Outer Fold 4 ---\n","Best params (fold 4): {'alpha': np.float64(0.0004982752357076451)}\n","Fold 4 R²: 0.4066 | RMSE: 0.1166 | MAE: 0.3109 | MAX ERROR: 0.6914\n","\n","--- Outer Fold 5 ---\n","Best params (fold 5): {'alpha': np.float64(0.0004982752357076451)}\n","Fold 5 R²: 0.3453 | RMSE: 0.1279 | MAE: 0.2953 | MAX ERROR: 1.3230\n","\n","Mean Outer R²: 0.4314\n","Mean Outer RMSE: 0.1153\n","Mean Outer MAE: 0.2901\n","Mean Outer Max Error: 0.9183\n","Most common/best parameters across folds: {'alpha': 0.0004982752357076451}\n","\n","===== SGD Regressor (squared_epsilon_insensitive) =====\n","\n","--- Outer Fold 1 ---\n","Best params (fold 1): {'alpha': np.float64(0.0003666421832063722), 'epsilon': np.float64(0.03744721755761247), 'learning_rate': 'adaptive', 'penalty': 'l2'}\n","Fold 1 R²: 0.4947 | RMSE: 0.1042 | MAE: 0.2777 | MAX ERROR: 0.7194\n","\n","--- Outer Fold 2 ---\n","Best params (fold 2): {'alpha': np.float64(0.0003666421832063722), 'epsilon': np.float64(0.03744721755761247), 'learning_rate': 'adaptive', 'penalty': 'l2'}\n","Fold 2 R²: 0.5679 | RMSE: 0.0834 | MAE: 0.2447 | MAX ERROR: 0.6226\n","\n","--- Outer Fold 3 ---\n","Best params (fold 3): {'alpha': np.float64(0.0003666421832063722), 'epsilon': np.float64(0.03744721755761247), 'learning_rate': 'adaptive', 'penalty': 'l2'}\n","Fold 3 R²: 0.5332 | RMSE: 0.1044 | MAE: 0.2867 | MAX ERROR: 0.7151\n","\n","--- Outer Fold 4 ---\n","Best params (fold 4): {'alpha': np.float64(0.0003666421832063722), 'epsilon': np.float64(0.03744721755761247), 'learning_rate': 'adaptive', 'penalty': 'l2'}\n","Fold 4 R²: 0.4880 | RMSE: 0.1006 | MAE: 0.2851 | MAX ERROR: 0.6475\n","\n","--- Outer Fold 5 ---\n","Best params (fold 5): {'alpha': np.float64(0.0003666421832063722), 'epsilon': np.float64(0.03744721755761247), 'learning_rate': 'adaptive', 'penalty': 'l2'}\n","Fold 5 R²: 0.4446 | RMSE: 0.1085 | MAE: 0.2898 | MAX ERROR: 0.8114\n","\n","Mean Outer R²: 0.5057\n","Mean Outer RMSE: 0.1002\n","Mean Outer MAE: 0.2768\n","Mean Outer Max Error: 0.7032\n","Most common/best parameters across folds: {'alpha': 0.0003666421832063722, 'epsilon': 0.03744721755761247, 'learning_rate': 'adaptive', 'penalty': 'l2'}\n","\n","===== Nested CV Summary =====\n","\n","Linear Regression:\n","  Mean R² = 0.4157\n","  Mean RMSE = 0.1182\n","  Mean MAE = 0.2730\n"]},{"output_type":"error","ename":"KeyError","evalue":"'mean_max_err'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1094473745.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  Mean RMSE = {res['mean_rmse']:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  Mean MAE = {res['mean_mae']:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  Mean Max Error = {res['mean_max_err']:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean_best_params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  Representative Best Params: {res['mean_best_params']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'mean_max_err'"]}]},{"cell_type":"markdown","source":["DIMENSION 2"],"metadata":{"id":"o2kKITTydc4K"}},{"cell_type":"code","source":["####REMOVES RF AND PRESENTS HYPERPARAMETERS\n","\n","####CODE REMOVES RF AND DISPLAYS HYPERPARAMETERS\n","\n","from sklearn.linear_model import LinearRegression, Ridge, Lasso, SGDRegressor\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.model_selection import KFold, RandomizedSearchCV\n","from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, max_error\n","from scipy.stats import loguniform, uniform\n","import numpy as np\n","import pandas as pd\n","\n","# === Example data ===\n","# Replace with your actual data\n","X = X_sparse\n","y = y1\n","\n","# === Define models and parameter search spaces ===\n","param_distributions = {\n","    'Linear Regression': {\n","        'model': LinearRegression(),\n","        'params': {}  # No hyperparameters to tune\n","    },\n","    'Ridge Regression': {\n","        'model': Ridge(),\n","        'params': {'alpha': loguniform(1e-3, 1e3)}\n","    },\n","    'Lasso Regression': {\n","        'model': Lasso(max_iter=5000),\n","        'params': {'alpha': loguniform(1e-4, 10)}\n","    },\n","    'SGD Regressor (squared_epsilon_insensitive)': {\n","        'model': SGDRegressor(\n","            loss='squared_epsilon_insensitive',\n","            max_iter=2000,\n","            random_state=42\n","        ),\n","        'params': {\n","            'alpha': loguniform(1e-5, 1e-1),\n","            'penalty': ['l1', 'l2', 'elasticnet'],\n","            'epsilon': uniform(0.001, 0.2),\n","            'learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive']\n","        }\n","    }\n","}\n","\n","# === Cross-validation strategies ===\n","outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n","inner_cv = KFold(n_splits=4, shuffle=True, random_state=42)\n","\n","# === Storage for results ===\n","nested_results = {}\n","\n","# === Nested Cross-Validation ===\n","for name, cfg in param_distributions.items():\n","    print(f\"\\n===== {name} =====\")\n","    outer_scores = []\n","    best_params_per_fold = []\n","\n","    for fold, (train_idx, test_idx) in enumerate(outer_cv.split(X)):\n","        print(f\"\\n--- Outer Fold {fold + 1} ---\")\n","\n","        X_train, X_test = X[train_idx], X[test_idx]\n","        y_train, y_test = y[train_idx], y[test_idx]\n","\n","        if cfg['params']:\n","            search = RandomizedSearchCV(\n","                estimator=cfg['model'],\n","                param_distributions=cfg['params'],\n","                n_iter=30,\n","                scoring='r2',\n","                cv=inner_cv,\n","                n_jobs=-1,\n","                random_state=42,\n","                verbose=0\n","            )\n","            search.fit(X_train, y_train)\n","            best_model = search.best_estimator_\n","            best_params = search.best_params_\n","            print(f\"Best params (fold {fold + 1}): {best_params}\")\n","        else:\n","            best_model = cfg['model']\n","            best_model.fit(X_train, y_train)\n","            best_params = {}\n","            print(f\"No hyperparameters to tune for {name}.\")\n","\n","        best_params_per_fold.append(best_params)\n","\n","        # Evaluate on outer test set\n","        y_pred = best_model.predict(X_test)\n","        r2 = r2_score(y_test, y_pred)\n","        rmse = mean_squared_error(y_test, y_pred)\n","        mae = mean_absolute_error(y_test, y_pred)\n","        max_err = max_error(y_test, y_pred)\n","\n","        outer_scores.append({\n","            'fold': fold + 1,\n","            'r2': r2,\n","            'rmse': rmse,\n","            'mae': mae,\n","            'max_error': max_err\n","        })\n","\n","        print(f\"Fold {fold + 1} R²: {r2:.4f} | RMSE: {rmse:.4f} | MAE: {mae:.4f} | Max Err: {max_err:.4f}\")\n","\n","    # Aggregate outer CV results\n","    mean_r2 = np.mean([r['r2'] for r in outer_scores])\n","    mean_rmse = np.mean([r['rmse'] for r in outer_scores])\n","    mean_mae = np.mean([r['mae'] for r in outer_scores])\n","    mean_maxerr = np.mean([r['max_error'] for r in outer_scores])\n","\n","    # Summarize best params (if any)\n","    if best_params_per_fold and any(best_params_per_fold):\n","        params_df = pd.DataFrame(best_params_per_fold)\n","        mean_params = params_df.mode().iloc[0].to_dict()  # most common best params\n","    else:\n","        mean_params = {}\n","\n","    nested_results[name] = {\n","        'outer_scores': outer_scores,\n","        'mean_r2': mean_r2,\n","        'mean_rmse': mean_rmse,\n","        'mean_mae': mean_mae,\n","        'mean_max_error': mean_maxerr,\n","        'best_params_per_fold': best_params_per_fold,\n","        'mean_best_params': mean_params\n","    }\n","\n","    print(f\"\\nMean Outer R²: {mean_r2:.4f}\")\n","    print(f\"Mean Outer RMSE: {mean_rmse:.4f}\")\n","    print(f\"Mean Outer MAE: {mean_mae:.4f}\")\n","    print(f\"Mean Outer Max Error: {mean_maxerr:.4f}\")\n","    if mean_params:\n","        print(f\"Most common/best parameters across folds: {mean_params}\")\n","\n","# === Summary of all models ===\n","print(\"\\n===== Nested CV Summary =====\")\n","for name, res in nested_results.items():\n","    print(f\"\\n{name}:\")\n","    print(f\"  Mean R² = {res['mean_r2']:.4f}\")\n","    print(f\"  Mean RMSE = {res['mean_rmse']:.4f}\")\n","    print(f\"  Mean MAE = {res['mean_mae']:.4f}\")\n","    if res['mean_best_params']:\n","        print(f\"  Representative Best Params: {res['mean_best_params']}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2dCuajdQx9VN","executionInfo":{"status":"ok","timestamp":1763420994832,"user_tz":300,"elapsed":124863,"user":{"displayName":"Michael Hammer","userId":"02778246655929855865"}},"outputId":"0bac21e5-f29a-4679-9a0f-09300fccd11a"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","===== Linear Regression =====\n","\n","--- Outer Fold 1 ---\n","No hyperparameters to tune for Linear Regression.\n","Fold 1 R²: -0.2981 | RMSE: 0.0702 | MAE: 0.2165 | Max Err: 0.6202\n","\n","--- Outer Fold 2 ---\n","No hyperparameters to tune for Linear Regression.\n","Fold 2 R²: -0.5018 | RMSE: 0.1008 | MAE: 0.2536 | Max Err: 0.7640\n","\n","--- Outer Fold 3 ---\n","No hyperparameters to tune for Linear Regression.\n","Fold 3 R²: 0.0296 | RMSE: 0.0802 | MAE: 0.2291 | Max Err: 0.7769\n","\n","--- Outer Fold 4 ---\n","No hyperparameters to tune for Linear Regression.\n","Fold 4 R²: -0.1813 | RMSE: 0.0720 | MAE: 0.2109 | Max Err: 0.7522\n","\n","--- Outer Fold 5 ---\n","No hyperparameters to tune for Linear Regression.\n","Fold 5 R²: -0.2712 | RMSE: 0.0890 | MAE: 0.2406 | Max Err: 0.6842\n","\n","Mean Outer R²: -0.2446\n","Mean Outer RMSE: 0.0824\n","Mean Outer MAE: 0.2301\n","Mean Outer Max Error: 0.7195\n","\n","===== Ridge Regression =====\n","\n","--- Outer Fold 1 ---\n","Best params (fold 1): {'alpha': np.float64(0.3905441275210791)}\n","Fold 1 R²: 0.1412 | RMSE: 0.0465 | MAE: 0.1709 | Max Err: 0.5325\n","\n","--- Outer Fold 2 ---\n","Best params (fold 2): {'alpha': np.float64(0.3905441275210791)}\n","Fold 2 R²: 0.1057 | RMSE: 0.0601 | MAE: 0.2006 | Max Err: 0.6361\n","\n","--- Outer Fold 3 ---\n","Best params (fold 3): {'alpha': np.float64(0.3905441275210791)}\n","Fold 3 R²: 0.1377 | RMSE: 0.0712 | MAE: 0.2039 | Max Err: 0.8403\n","\n","--- Outer Fold 4 ---\n","Best params (fold 4): {'alpha': np.float64(0.5450293694558254)}\n","Fold 4 R²: 0.2173 | RMSE: 0.0477 | MAE: 0.1660 | Max Err: 0.5394\n","\n","--- Outer Fold 5 ---\n","Best params (fold 5): {'alpha': np.float64(0.5450293694558254)}\n","Fold 5 R²: 0.2457 | RMSE: 0.0528 | MAE: 0.1871 | Max Err: 0.5788\n","\n","Mean Outer R²: 0.1695\n","Mean Outer RMSE: 0.0557\n","Mean Outer MAE: 0.1857\n","Mean Outer Max Error: 0.6254\n","Most common/best parameters across folds: {'alpha': 0.3905441275210791}\n","\n","===== Lasso Regression =====\n","\n","--- Outer Fold 1 ---\n","Best params (fold 1): {'alpha': np.float64(0.0004982752357076451)}\n","Fold 1 R²: 0.0697 | RMSE: 0.0503 | MAE: 0.1813 | Max Err: 0.5445\n","\n","--- Outer Fold 2 ---\n","Best params (fold 2): {'alpha': np.float64(0.0006026889128682511)}\n","Fold 2 R²: 0.0778 | RMSE: 0.0619 | MAE: 0.2094 | Max Err: 0.5776\n","\n","--- Outer Fold 3 ---\n","Best params (fold 3): {'alpha': np.float64(0.0004982752357076451)}\n","Fold 3 R²: 0.1107 | RMSE: 0.0735 | MAE: 0.2091 | Max Err: 0.7340\n","\n","--- Outer Fold 4 ---\n","Best params (fold 4): {'alpha': np.float64(0.0004982752357076451)}\n","Fold 4 R²: 0.1217 | RMSE: 0.0536 | MAE: 0.1783 | Max Err: 0.5735\n","\n","--- Outer Fold 5 ---\n","Best params (fold 5): {'alpha': np.float64(0.0009962513222055108)}\n","Fold 5 R²: 0.0778 | RMSE: 0.0645 | MAE: 0.2135 | Max Err: 0.5542\n","\n","Mean Outer R²: 0.0915\n","Mean Outer RMSE: 0.0608\n","Mean Outer MAE: 0.1983\n","Mean Outer Max Error: 0.5967\n","Most common/best parameters across folds: {'alpha': 0.0004982752357076451}\n","\n","===== SGD Regressor (squared_epsilon_insensitive) =====\n","\n","--- Outer Fold 1 ---\n","Best params (fold 1): {'alpha': np.float64(0.0003666421832063722), 'epsilon': np.float64(0.03744721755761247), 'learning_rate': 'adaptive', 'penalty': 'l2'}\n","Fold 1 R²: 0.1093 | RMSE: 0.0482 | MAE: 0.1749 | Max Err: 0.5104\n","\n","--- Outer Fold 2 ---\n","Best params (fold 2): {'alpha': np.float64(0.0003666421832063722), 'epsilon': np.float64(0.03744721755761247), 'learning_rate': 'adaptive', 'penalty': 'l2'}\n","Fold 2 R²: 0.0836 | RMSE: 0.0615 | MAE: 0.2067 | Max Err: 0.5948\n","\n","--- Outer Fold 3 ---\n","Best params (fold 3): {'alpha': np.float64(0.0003666421832063722), 'epsilon': np.float64(0.03744721755761247), 'learning_rate': 'adaptive', 'penalty': 'l2'}\n","Fold 3 R²: 0.1103 | RMSE: 0.0735 | MAE: 0.2047 | Max Err: 0.8000\n","\n","--- Outer Fold 4 ---\n","Best params (fold 4): {'alpha': np.float64(0.0003666421832063722), 'epsilon': np.float64(0.03744721755761247), 'learning_rate': 'adaptive', 'penalty': 'l2'}\n","Fold 4 R²: 0.1794 | RMSE: 0.0500 | MAE: 0.1719 | Max Err: 0.5515\n","\n","--- Outer Fold 5 ---\n","Best params (fold 5): {'alpha': np.float64(0.0003666421832063722), 'epsilon': np.float64(0.03744721755761247), 'learning_rate': 'adaptive', 'penalty': 'l2'}\n","Fold 5 R²: 0.1941 | RMSE: 0.0564 | MAE: 0.1949 | Max Err: 0.5727\n","\n","Mean Outer R²: 0.1353\n","Mean Outer RMSE: 0.0579\n","Mean Outer MAE: 0.1906\n","Mean Outer Max Error: 0.6059\n","Most common/best parameters across folds: {'alpha': 0.0003666421832063722, 'epsilon': 0.03744721755761247, 'learning_rate': 'adaptive', 'penalty': 'l2'}\n","\n","===== Nested CV Summary =====\n","\n","Linear Regression:\n","  Mean R² = -0.2446\n","  Mean RMSE = 0.0824\n","  Mean MAE = 0.2301\n","\n","Ridge Regression:\n","  Mean R² = 0.1695\n","  Mean RMSE = 0.0557\n","  Mean MAE = 0.1857\n","  Representative Best Params: {'alpha': 0.3905441275210791}\n","\n","Lasso Regression:\n","  Mean R² = 0.0915\n","  Mean RMSE = 0.0608\n","  Mean MAE = 0.1983\n","  Representative Best Params: {'alpha': 0.0004982752357076451}\n","\n","SGD Regressor (squared_epsilon_insensitive):\n","  Mean R² = 0.1353\n","  Mean RMSE = 0.0579\n","  Mean MAE = 0.1906\n","  Representative Best Params: {'alpha': 0.0003666421832063722, 'epsilon': 0.03744721755761247, 'learning_rate': 'adaptive', 'penalty': 'l2'}\n"]}]},{"cell_type":"markdown","source":["FEATURE EXTRACTION - DIMENSION 1"],"metadata":{"id":"YOtmkPinI2n9"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.linear_model import Ridge, Lasso, SGDRegressor, LinearRegression\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.model_selection import KFold, RandomizedSearchCV\n","from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, max_error\n","\n","X = X_sparse\n","y = ydim1\n","\n","\n","feature_names = np.array(vectorizer.get_feature_names_out())\n","\n","models = {\n","    'Linear Regression': LinearRegression(),\n","    'Ridge Regression': Ridge(alpha=0.05589),\n","    'Lasso Regression': Lasso(max_iter=5000, alpha=0.000498),\n","    'SGD Regressor (squared_epsilon_insensitive)': SGDRegressor(\n","        loss='squared_epsilon_insensitive',\n","        max_iter=2000,\n","        random_state=42,\n","        alpha=0.000366,\n","        penalty='l2',\n","        epsilon=0.0374,\n","        learning_rate='adaptive'\n","    )\n","}\n","\n","for name, model in models.items():\n","    print(f\"Training {name}...\")\n","    model.fit(X, y)\n","    coefs = model.coef_\n","\n","    # Sort coefficients\n","    sorted_idx = np.argsort(coefs)\n","    top_neg_idx = sorted_idx[:20]   # 5 most negative\n","    top_pos_idx = sorted_idx[-20:]  # 5 most positive\n","\n","    print(f\"\\n=== {name} ===\")\n","    print(\"Top positive words (increase Dim 1 score):\")\n","    for i in top_pos_idx[::-1]:\n","        print(f\"  {feature_names[i]:<15} {coefs[i]:.4f}\")\n","\n","    print(\"Top negative words (decrease Dim 1 score):\")\n","    for i in top_neg_idx:\n","        print(f\"  {feature_names[i]:<15} {coefs[i]:.4f}\")\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RL8KxCmTgYF9","executionInfo":{"status":"ok","timestamp":1763356709199,"user_tz":300,"elapsed":1745,"user":{"displayName":"Michael Hammer","userId":"02778246655929855865"}},"outputId":"412a08bc-9053-4ebe-c34b-c76c1ffee804"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Linear Regression...\n","\n","=== Linear Regression ===\n","Top positive words (increase Dim 1 score):\n","  statehood       0.6347\n","  lodge           0.5522\n","  taiwan          0.5386\n","  sergeant        0.5326\n","  331             0.5240\n","  veterans        0.5228\n","  unborn          0.4785\n","  bureaucrat      0.4769\n","  epa             0.4538\n","  missile         0.4364\n","  obamacare       0.4123\n","  trillion        0.4017\n","  illegal         0.3994\n","  dad             0.3978\n","  enemy           0.3955\n","  alien           0.3941\n","  patent          0.3890\n","  coach           0.3809\n","  liberty         0.3624\n","  fathers         0.3542\n","Top negative words (decrease Dim 1 score):\n","  pollution       -0.5035\n","  borrower        -0.4888\n","  aca             -0.4828\n","  rail            -0.4731\n","  print           -0.4430\n","  bureau          -0.4146\n","  passenger       -0.3964\n","  wall            -0.3887\n","  los             -0.3883\n","  climate         -0.3859\n","  loophole        -0.3849\n","  wage            -0.3533\n","  houston         -0.3527\n","  fishing         -0.3508\n","  philadelphia    -0.3413\n","  confederate     -0.3311\n","  genocide        -0.3235\n","  angeles         -0.3229\n","  voting          -0.3187\n","  prescription    -0.3175\n","Training Ridge Regression...\n","\n","=== Ridge Regression ===\n","Top positive words (increase Dim 1 score):\n","  statehood       0.5547\n","  sergeant        0.4896\n","  bureaucrat      0.4777\n","  taiwan          0.4610\n","  unborn          0.4583\n","  lodge           0.4455\n","  epa             0.4307\n","  obamacare       0.4253\n","  331             0.3986\n","  veterans        0.3958\n","  missile         0.3923\n","  illegal         0.3713\n","  trillion        0.3625\n","  enemy           0.3479\n","  coach           0.3450\n","  alien           0.3423\n","  dad             0.3369\n","  liberty         0.3367\n","  patent          0.3248\n","  fathers         0.3205\n","Top negative words (decrease Dim 1 score):\n","  print           -0.5128\n","  aca             -0.4581\n","  rail            -0.4511\n","  borrower        -0.4509\n","  pollution       -0.4505\n","  climate         -0.4044\n","  los             -0.3921\n","  loophole        -0.3738\n","  bureau          -0.3734\n","  passenger       -0.3570\n","  philadelphia    -0.3527\n","  genocide        -0.3469\n","  wall            -0.3468\n","  voting          -0.3451\n","  fishing         -0.3426\n","  wage            -0.3420\n","  tweet           -0.3375\n","  angeles         -0.3352\n","  african         -0.3344\n","  cuba            -0.3268\n","Training Lasso Regression...\n","\n","=== Lasso Regression ===\n","Top positive words (increase Dim 1 score):\n","  bureaucrat      2.3540\n","  unborn          1.5260\n","  obamacare       1.1880\n","  epa             1.0124\n","  sergeant        0.9419\n","  enemy           0.8550\n","  trillion        0.7606\n","  coach           0.6618\n","  liberty         0.6616\n","  god             0.6482\n","  missile         0.5703\n","  illegal         0.5515\n","  customer        0.5141\n","  amnesty         0.4970\n","  email           0.4327\n","  fairtax         0.4095\n","  statehood       0.3947\n","  hemp            0.3880\n","  irs             0.3746\n","  branch          0.3375\n","Top negative words (decrease Dim 1 score):\n","  aca             -2.0630\n","  loophole        -1.7932\n","  reproductive    -1.5481\n","  los             -1.5460\n","  borrower        -1.4444\n","  toxic           -1.3314\n","  wage            -1.2867\n","  silence         -1.2090\n","  shutdown        -1.2037\n","  voting          -1.0987\n","  climate         -1.0035\n","  african         -0.8121\n","  pollution       -0.7685\n","  rape            -0.7313\n","  genocide        -0.7174\n","  muhammad        -0.6985\n","  poverty         -0.6981\n","  rail            -0.6755\n","  cuomo           -0.6409\n","  democracy       -0.6045\n","Training SGD Regressor (squared_epsilon_insensitive)...\n","\n","=== SGD Regressor (squared_epsilon_insensitive) ===\n","Top positive words (increase Dim 1 score):\n","  obamacare       0.4407\n","  epa             0.4126\n","  irs             0.4125\n","  unborn          0.3996\n","  bureaucrat      0.3355\n","  sergeant        0.3217\n","  trillion        0.3148\n","  baby            0.3067\n","  god             0.2692\n","  illegal         0.2691\n","  amnesty         0.2617\n","  branch          0.2493\n","  alien           0.2462\n","  overreach       0.2387\n","  liberty         0.2271\n","  taiwan          0.2265\n","  trafficking     0.2264\n","  331             0.2242\n","  email           0.2232\n","  coach           0.2177\n","Top negative words (decrease Dim 1 score):\n","  climate         -0.4554\n","  african         -0.3623\n","  voting          -0.3559\n","  flint           -0.2956\n","  pollution       -0.2824\n","  caucus          -0.2818\n","  aca             -0.2704\n","  background      -0.2674\n","  loophole        -0.2662\n","  wage            -0.2645\n","  poverty         -0.2609\n","  rights          -0.2404\n","  lgbt            -0.2393\n","  rider           -0.2292\n","  shooting        -0.2292\n","  los             -0.2236\n","  pesticide       -0.2210\n","  silence         -0.2194\n","  genocide        -0.2109\n","  arbitration     -0.2091\n"]}]},{"cell_type":"markdown","source":["FEATURE EXTRACTION - DIMENSION 2"],"metadata":{"id":"EhRvtAIoI__Q"}},{"cell_type":"code","source":["####FEATURE EXTRACTION -- DIMENSION 2\n","\n","import pandas as pd\n","import numpy as np\n","from sklearn.linear_model import Ridge, Lasso, SGDRegressor, LinearRegression\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.model_selection import KFold, RandomizedSearchCV\n","from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, max_error\n","\n","X = X_sparse\n","y = y1\n","\n","feature_names = np.array(vectorizer.get_feature_names_out())\n","\n","models = {\n","    'Linear Regression': LinearRegression(),\n","    'Ridge Regression': Ridge(alpha=0.05589),\n","    'Lasso Regression': Lasso(max_iter=5000, alpha=0.000498),\n","    'SGD Regressor (squared_epsilon_insensitive)': SGDRegressor(\n","        loss='squared_epsilon_insensitive',\n","        max_iter=2000,\n","        random_state=42,\n","        alpha=0.000366,\n","        penalty='l2',\n","        epsilon=0.0374,\n","        learning_rate='adaptive'\n","    )\n","}\n","\n","for name, model in models.items():\n","    print(f\"Training {name}...\")\n","    model.fit(X, y1)\n","    coefs = model.coef_\n","\n","    # Sort coefficients\n","    sorted_idx = np.argsort(coefs)\n","    top_neg_idx = sorted_idx[:20]   # 5 most negative\n","    top_pos_idx = sorted_idx[-20:]  # 5 most positive\n","\n","    print(f\"\\n=== {name} ===\")\n","    print(\"Top positive words (increase Dim 1 score):\")\n","    for i in top_pos_idx[::-1]:\n","        print(f\"  {feature_names[i]:<15} {coefs[i]:.4f}\")\n","\n","    print(\"Top negative words (decrease Dim 1 score):\")\n","    for i in top_neg_idx:\n","        print(f\"  {feature_names[i]:<15} {coefs[i]:.4f}\")\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763356822968,"user_tz":300,"elapsed":929,"user":{"displayName":"Michael Hammer","userId":"02778246655929855865"}},"outputId":"1d990b62-5f3b-46c6-813e-3549d32ac2bf","id":"r3m2iiosmEpK"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Linear Regression...\n","\n","=== Linear Regression ===\n","Top positive words (increase Dim 1 score):\n","  print           0.6145\n","  rural           0.5713\n","  farm            0.5500\n","  housing         0.4744\n","  irs             0.4644\n","  orleans         0.4501\n","  intelligence    0.4356\n","  cftc            0.4302\n","  bashar          0.4189\n","  memorandum      0.4108\n","  alassad         0.4072\n","  tweet           0.4065\n","  omaha           0.4028\n","  veterans        0.3937\n","  valley          0.3932\n","  houston         0.3901\n","  classified      0.3872\n","  331             0.3799\n","  november        0.3781\n","  cyber           0.3685\n","Top negative words (decrease Dim 1 score):\n","  carbon          -0.4651\n","  northern        -0.3982\n","  progressive     -0.3786\n","  marijuana       -0.3105\n","  gmo             -0.3074\n","  marriage        -0.3070\n","  hemp            -0.3009\n","  subsidy         -0.2922\n","  chemical        -0.2721\n","  conservation    -0.2702\n","  contractor      -0.2686\n","  osc             -0.2683\n","  corporate       -0.2620\n","  pollution       -0.2608\n","  sergeant        -0.2548\n","  encryption      -0.2514\n","  device          -0.2424\n","  defendant       -0.2423\n","  chester         -0.2384\n","  inspector       -0.2377\n","Training Ridge Regression...\n","\n","=== Ridge Regression ===\n","Top positive words (increase Dim 1 score):\n","  farm            0.4239\n","  houston         0.3938\n","  housing         0.3874\n","  rural           0.3855\n","  cftc            0.3498\n","  omaha           0.3152\n","  orleans         0.3136\n","  irs             0.2979\n","  intelligence    0.2927\n","  memorandum      0.2895\n","  bashar          0.2865\n","  classified      0.2795\n","  valley          0.2765\n","  alassad         0.2739\n","  print           0.2662\n","  dyslexia        0.2649\n","  organ           0.2604\n","  fort            0.2600\n","  november        0.2588\n","  medal           0.2572\n","Top negative words (decrease Dim 1 score):\n","  marriage        -0.3766\n","  hemp            -0.3688\n","  subsidy         -0.3285\n","  progressive     -0.3239\n","  carbon          -0.3108\n","  sergeant        -0.3027\n","  marijuana       -0.2944\n","  northern        -0.2936\n","  defendant       -0.2653\n","  corporate       -0.2586\n","  osc             -0.2503\n","  encryption      -0.2490\n","  receipt         -0.2485\n","  gmo             -0.2433\n","  conservation    -0.2392\n","  cap             -0.2346\n","  device          -0.2296\n","  contractor      -0.2250\n","  maritime        -0.2236\n","  unrelated       -0.2209\n","Training Lasso Regression...\n","\n","=== Lasso Regression ===\n","Top positive words (increase Dim 1 score):\n","  farm            0.7467\n","  rural           0.5552\n","  houston         0.4607\n","  agriculture     0.4034\n","  housing         0.3128\n","  appropriations  0.2945\n","  unborn          0.2875\n","  cftc            0.2528\n","  omaha           0.2165\n","  cyber           0.1754\n","  fort            0.1650\n","  obamacare       0.1550\n","  irs             0.1489\n","  epa             0.1043\n","  cbp             0.0858\n","  memorandum      0.0511\n","  chattanooga     0.0381\n","  forest          0.0362\n","  orleans         0.0295\n","  en              0.0282\n","Top negative words (decrease Dim 1 score):\n","  subsidy         -1.3152\n","  corporate       -1.1527\n","  corporation     -0.6213\n","  hemp            -0.5941\n","  progressive     -0.4893\n","  climate         -0.4844\n","  trillion        -0.3924\n","  marriage        -0.3671\n","  marijuana       -0.3096\n","  rape            -0.3052\n","  defendant       -0.2905\n","  inquiry         -0.2701\n","  puerto          -0.2588\n","  sergeant        -0.2333\n","  cancer          -0.2162\n","  fishery         -0.1854\n","  afghanistan     -0.1711\n","  ethiopia        -0.1698\n","  gmo             -0.1665\n","  conservation    -0.1656\n","Training SGD Regressor (squared_epsilon_insensitive)...\n","\n","=== SGD Regressor (squared_epsilon_insensitive) ===\n","Top positive words (increase Dim 1 score):\n","  farm            0.1882\n","  rural           0.1619\n","  housing         0.1558\n","  houston         0.1511\n","  agriculture     0.1361\n","  irs             0.1268\n","  cftc            0.1216\n","  cyber           0.1112\n","  producer        0.1083\n","  omaha           0.1065\n","  appropriations  0.1049\n","  forest          0.0984\n","  farmer          0.0968\n","  memorandum      0.0939\n","  fort            0.0938\n","  obamacare       0.0929\n","  unborn          0.0928\n","  cfpb            0.0912\n","  orleans         0.0878\n","  gold            0.0853\n","Top negative words (decrease Dim 1 score):\n","  climate         -0.1895\n","  hemp            -0.1647\n","  marijuana       -0.1403\n","  puerto          -0.1386\n","  corporation     -0.1324\n","  progressive     -0.1225\n","  corporate       -0.1202\n","  wage            -0.1163\n","  marriage        -0.1159\n","  afghanistan     -0.1146\n","  trillion        -0.1120\n","  subsidy         -0.1117\n","  ethiopia        -0.1106\n","  sergeant        -0.1093\n","  maritime        -0.1076\n","  coastal         -0.1068\n","  flint           -0.1068\n","  fishery         -0.1052\n","  defendant       -0.1000\n","  taiwan          -0.0967\n"]}]}]}