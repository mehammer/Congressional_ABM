{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNCRx4dHZDjDEhtxU29/vEx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["3/8/25: Updated from the last stable/functional version (\"dim1_BOW_train-test-combined\").  Two major additions:  A different cross-validation method that does not require using **negative** mean absolute error.  I have also updated and expanded the \"newsroom\" test dataset in the hope of resolving the negative R2 issue.\n","\n","7/14/25: Implemented HK request for word2vec"],"metadata":{"id":"roHAooeV_MoE"}},{"cell_type":"code","source":["!pip install gensim"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UcrjwvZW5-P9","executionInfo":{"status":"ok","timestamp":1759812918189,"user_tz":240,"elapsed":13785,"user":{"displayName":"Michael Hammer","userId":"02778246655929855865"}},"outputId":"430d3641-cd3d-4e86-b4be-2351311b712e"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.3.3)\n","Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.26.4)\n","Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.13.1)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.1)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"VfZNy7ZeGGAl","executionInfo":{"status":"ok","timestamp":1759812928335,"user_tz":240,"elapsed":2213,"user":{"displayName":"Michael Hammer","userId":"02778246655929855865"}}},"outputs":[],"source":["##import and format data\n","import pandas as pd\n","import numpy as np\n","\n","#Upload complete training dataset\n","all114 = pd.read_csv('training_data_114_final.csv',dtype='string')\n","all2 = all114.astype({'speech':'string','nominate_dim1':'float', 'nominate_dim2': 'float'})\n","final114 = all2.dropna()\n"]},{"cell_type":"markdown","source":[],"metadata":{"id":"wAAw9SyFcP7H"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"iozZpiKEG6DR","executionInfo":{"status":"ok","timestamp":1759812932645,"user_tz":240,"elapsed":57,"user":{"displayName":"Michael Hammer","userId":"02778246655929855865"}}},"outputs":[],"source":["#upload the custom stopword list\n","from congress_stopwords import congress\n"]},{"cell_type":"code","source":["final114.speech.str.len()\n","#Just making sure that no speech data were truncated during preprocessing\n","#and that the proper number of documents are present."],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":458},"id":"emwjBiW8NfsT","executionInfo":{"status":"ok","timestamp":1759812934462,"user_tz":240,"elapsed":42,"user":{"displayName":"Michael Hammer","userId":"02778246655929855865"}},"outputId":"bd02d855-730d-43c3-ebd4-0c4286b967e9","collapsed":true},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0       17784\n","1       25356\n","2      113985\n","3      109704\n","4       45340\n","        ...  \n","433     90749\n","434     31889\n","435     15301\n","436     24527\n","437      9209\n","Name: speech, Length: 438, dtype: Int64"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>speech</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>17784</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>25356</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>113985</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>109704</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>45340</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>433</th>\n","      <td>90749</td>\n","    </tr>\n","    <tr>\n","      <th>434</th>\n","      <td>31889</td>\n","    </tr>\n","    <tr>\n","      <th>435</th>\n","      <td>15301</td>\n","    </tr>\n","    <tr>\n","      <th>436</th>\n","      <td>24527</td>\n","    </tr>\n","    <tr>\n","      <th>437</th>\n","      <td>9209</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>438 rows Ã— 1 columns</p>\n","</div><br><label><b>dtype:</b> Int64</label>"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["import numpy as np\n","from gensim.utils import simple_preprocess\n","from gensim.models import Word2Vec\n","\n","# 1. Tokenize speeches\n","tokenized_sentences = [simple_preprocess(speech) for speech in final114.speech]\n","\n","# 2. Train Word2Vec model\n","#tried min_count of 5, 10 -- higher number increased max error, didn't up r2\n","w2v_model = Word2Vec(\n","    sentences=tokenized_sentences,\n","    vector_size=100,    # dimensionality of word vectors\n","    window=5,           # context window size\n","    min_count=5,       # ignore rare words\n","    workers=4,          # number of threads\n","    sg=1                # 1 = skip-gram; 0 = CBOW\n",")\n","\n","# 3. Define a function to average word vectors in a document\n","def document_vector(doc, model):\n","    words = [word for word in doc if word in model.wv]\n","    if not words:\n","        return np.zeros(model.vector_size)\n","    return np.mean([model.wv[word] for word in words], axis=0)\n","\n","# 4. Transform all speeches into averaged word vectors\n","X = np.array([document_vector(doc, w2v_model) for doc in tokenized_sentences])\n","\n","# 5. Target variables\n","y = final114.nominate_dim1\n","y1 = final114.nominate_dim2\n"],"metadata":{"id":"-AeZSRESBIE5","executionInfo":{"status":"ok","timestamp":1759813361881,"user_tz":240,"elapsed":150309,"user":{"displayName":"Michael Hammer","userId":"02778246655929855865"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["#making sure I ended up with the right number of documents\n","len(X)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i5sP3l5SBcOJ","executionInfo":{"status":"ok","timestamp":1759813366688,"user_tz":240,"elapsed":20,"user":{"displayName":"Michael Hammer","userId":"02778246655929855865"}},"outputId":"ea891e8d-1ecc-473e-c635-a8928e72a875"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["438"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["#inspecting sample arrays\n","X[100:102]"],"metadata":{"id":"m5NzsTh1-4YC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import RepeatedKFold, cross_val_score, cross_val_predict\n","from sklearn.linear_model import LinearRegression\n","from sklearn.ensemble import GradientBoostingRegressor\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.model_selection import train_test_split, RepeatedKFold\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, max_error\n"],"metadata":{"id":"BwkU8QBTT1po","executionInfo":{"status":"ok","timestamp":1759813108984,"user_tz":240,"elapsed":1627,"user":{"displayName":"Michael Hammer","userId":"02778246655929855865"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["#TRAINING AND CROSS-VALIDATION -- DIMENSION 1\n","models = {\n","    'Linear Regression': LinearRegression(),\n","    #'Random Forest Regressor': RandomForestRegressor(n_estimators=600),\n","    #'Gradient Boosting Regressor': GradientBoostingRegressor(n_estimators=1600)\n","}\n","\n","# Create a RepeatedKFold cross-validator\n","rkf = RepeatedKFold(n_splits=4, n_repeats=1, random_state=42)\n","\n","# Function to calculate all metrics\n","def calculate_metrics(model, X, y, cv):\n","    r2_scores = []\n","    rmse_scores = []\n","    mae_scores = []\n","    max_errors = []\n","\n","    for train_index, test_index in cv.split(X):\n","        X_train, X_test = X[train_index], X[test_index]\n","        y_train, y_test = y[train_index], y[test_index]\n","\n","        model.fit(X_train, y_train)\n","        y_pred = model.predict(X_test)\n","\n","        # Calculate metrics\n","        r2_scores.append(r2_score(y_test, y_pred))\n","        rmse_scores.append(np.sqrt(mean_squared_error(y_test, y_pred)))\n","        mae_scores.append(mean_absolute_error(y_test, y_pred))\n","        max_errors.append(max_error(y_test, y_pred))\n","\n","    return r2_scores, rmse_scores, mae_scores, max_errors\n","\n","# Evaluate each model using cross-validation and calculate the metrics\n","for model_name, model in models.items():\n","    r2_scores, rmse_scores, mae_scores, max_errors = calculate_metrics(model, X, y, rkf)\n","\n","    #print(f\"{model_name} - R2 Scores: {r2_scores}\")\n","    print(f\"{model_name} - Mean R2: {np.mean(r2_scores)}\")\n","    #print(f\"{model_name} - RMSE Scores: {rmse_scores}\")\n","    print(f\"{model_name} - Mean RMSE: {np.mean(rmse_scores)}\")\n","    #print(f\"{model_name} - MAE Scores: {mae_scores}\")\n","    print(f\"{model_name} - Mean MAE: {np.mean(mae_scores)}\")\n","    #print(f\"{model_name} - Max Errors: {max_errors}\")\n","    print(f\"{model_name} - Mean Max Error: {np.mean(max_errors)}\\n\")\n"],"metadata":{"id":"ozYbeMjBO0cp"},"execution_count":null,"outputs":[]}]}